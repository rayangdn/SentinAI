{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a9bdbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: <module 'torch' from '/home/rayan/miniconda3/envs/sentinai_env/lib/python3.11/site-packages/torch/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# Check if PyTorch can see CUDA\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d755acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HateXplain dataset\n",
    "from datasets import load_dataset\n",
    "hatexplain = load_dataset(\"hatexplain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "153f2dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 15383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1922\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1924\n",
      "    })\n",
      "})\n",
      "\n",
      "Example from training set:\n",
      "{'id': '23107796_gab', 'annotators': {'label': [0, 2, 2], 'annotator_id': [203, 204, 233], 'target': [['Hindu', 'Islam'], ['Hindu', 'Islam'], ['Hindu', 'Islam', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry']}\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(hatexplain)\n",
    "\n",
    "# Display an example from the training set\n",
    "print(\"\\nExample from training set:\")\n",
    "print(hatexplain['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54066fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def process_dataset(data):\n",
    "    processed_splits = {}\n",
    "    \n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        split_data = []\n",
    "        for item in data[split]:\n",
    "            text = \" \".join(item[\"post_tokens\"])\n",
    "            labels = item[\"annotators\"][\"label\"]\n",
    "            label = Counter(labels).most_common(1)[0][0]\n",
    "            split_data.append({\"text\": text, \"label\": label})\n",
    "        \n",
    "        processed_splits[split] = Dataset.from_list(split_data)\n",
    "        \n",
    "    return DatasetDict(processed_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13277057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 6251, 0: 4748, 2: 4384})\n",
      "Counter({1: 781, 0: 593, 2: 548})\n",
      "Counter({1: 782, 0: 594, 2: 548})\n"
     ]
    }
   ],
   "source": [
    "# Process the train and validation data\n",
    "hatexplain_df = process_dataset(hatexplain)\n",
    "\n",
    "# Check class distribution \n",
    "print(Counter(hatexplain_df[\"train\"][\"label\"]))\n",
    "print(Counter(hatexplain_df[\"validation\"][\"label\"]))\n",
    "print(Counter(hatexplain_df[\"test\"][\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f76e809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiating  AutoTokenizer will directly create a class of the relevant architecture.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ed64003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "cache_files = {\"train\": \"~/.cache/hatexplain/hatexplain_train_tokenized.arrow\",\n",
    "               \"validation\": \"~/.cache/hatexplain/hatexplain_val_tokenized.arrow\",\n",
    "               \"test\": \"~/.cache/hatexplain/hatexplain_test_tokenized.arrow\"\n",
    "               }\n",
    "               \n",
    "# Path to the local cache files, where the current computation from the following function will be stored. \n",
    "# Caching saves RAM when working with large datasets and saves time instead of doing transformations on the fly.\n",
    "tokenized_hatexplain = hatexplain_df.map(tokenize_function, batched=True, cache_file_names=cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91b1a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_hatexplain = tokenized_hatexplain.remove_columns([\"text\"])\n",
    "tokenized_hatexplain = tokenized_hatexplain.rename_column(\"label\", \"labels\")\n",
    "tokenized_hatexplain.set_format(\"torch\")\n",
    "\n",
    "\n",
    "# create a DataLoader for your training and test datasets so you can iterate over batches of data:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_hatexplain[\"train\"], shuffle=True, batch_size=64)\n",
    "val_dataloader = DataLoader(tokenized_hatexplain[\"validation\"], shuffle=True, batch_size=64)\n",
    "test_dataloader = DataLoader(tokenized_hatexplain[\"test\"], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf1d06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"HATESPEECH\", 1: \"NORMAL\", 2: \"OFFENSIVE\"}\n",
    "label2id = {\"HATESPEECH\": 0, \"NORMAL\": 1, \"OFFENSIVE\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb3ec196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-tiny variant number of parameters:  4782851\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model_bert_l4 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/bert_uncased_L-4_H-128_A-2\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "print(\"bert-tiny variant number of parameters: \", model_bert_l4.num_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37ee99c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model_bert_l4.parameters(), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=1, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_bert_l4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f0df749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124c90c94271478b8f4e6f8a0cec9543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the tqdm library to add a progress bar over the number of training steps\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# put the model in train mode\n",
    "model_bert_l4.train()\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # iterate over batches in training set\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model_bert_l4(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763913df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e8d8bcdea64b8eb5b89efd384130ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6748178980228928}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# define the metric you want to use to evaluate your model\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "progress_bar = tqdm(range(len(val_dataloader)))\n",
    "\n",
    "# put the model in eval mode\n",
    "model_bert_l4.eval()\n",
    "# iterate over batches of evaluation dataset\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert_l4(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "\n",
    "    # use argmax to get the predicted class\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar.update(1)\n",
    "# calculate a metric by  calling metric.compute()\n",
    "metric.compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab36db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL\n",
      "NORMAL\n",
      "NORMAL\n"
     ]
    }
   ],
   "source": [
    "text_hate = \"People like you dont belong here. Go back to your country — you are ruining everything.\"\n",
    "text_normal = \"I just watched the new sci-fi movie last night. The visual effects were amazing!\"\n",
    "text_offensive = \"You're such an idiot, seriously. Do you ever think before you speak?\"\n",
    "text = [text_hate, text_normal, text_offensive]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\",  padding = True, truncation = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_bert_l4(**inputs.to(device)).logits\n",
    "\n",
    "\n",
    "#get the predicted id class\n",
    "predicted_class_id = logits.argmax(dim=1)\n",
    "\n",
    "# get the predicted class name\n",
    "for pred in predicted_class_id:\n",
    "  print(model_bert_l4.config.id2label[pred.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentinai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
